{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "import datetime\n",
    "\n",
    "#sns.set_style(style='darkgrid', rc=None)\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_preproc import *\n",
    "from functions_viz import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Import, interpolate and exponentially smooth Commitments of Traders data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CoT data for three asset classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data/US5YR_interpolated.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a8a7d75345e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import CoT data from .csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mus5yr_cot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data/US5YR_interpolated.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mspx_cot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data//SPX_interpolated.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdxy_cot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data//DXY_interpolated.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data/US5YR_interpolated.csv'"
     ]
    }
   ],
   "source": [
    "# import CoT data from .csv\n",
    "us5yr_cot = pd.read_csv('C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data/US5YR_interpolated.csv')\n",
    "spx_cot = pd.read_csv('C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data//SPX_interpolated.csv')\n",
    "dxy_cot = pd.read_csv('C:/Users/Tom/Google Drive/asset-price-prediction/CoT Data//DXY_interpolated.csv')\n",
    "\n",
    "# generate CoT dataframe from .csvs\n",
    "cot_indicators_us5yr = generate_cot_data(us5yr_cot)\n",
    "cot_indicators_spx = generate_cot_data(spx_cot)\n",
    "cot_indicators_dxa = generate_cot_data(dxy_cot)\n",
    "\n",
    "# drop more columns which are of no use:\n",
    "for df in [cot_indicators_us5yr, cot_indicators_spx,\n",
    "           cot_indicators_dxa]:\n",
    "    df = df.drop(\n",
    "        ['Pct_of_Open_Interest_All', 'Pct_of_Open_Interest_Old',\n",
    "         'Pct_of_Open_Interest_Other'],\n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create charts to demonstrate smoothed/unsmoothed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = interpolate_missing(spx_cot)\n",
    "# df2 = drop_unwanted_cols(df1)\n",
    "\n",
    "# # create a df of just floats\n",
    "# df_floats = df2.select_dtypes(include=['float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_charts(df_floats.iloc[:,0:6], dim1 = 2, dim2 = 3, fig_size = (10, 5), y_label=\"No. of Contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_charts(cot_indicators_spx.iloc[:,0:6], dim1 = 2, dim2 = 3, fig_size = (10, 5), y_label=\"No. of Contracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot all Commitment of Traderes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot S&P500 CoT charts\n",
    "plot_charts(cot_indicators_spx, dim1 = 10, dim2 = 8,\n",
    "            fig_size = (20, 20), y_label=\"No. of Contracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate new CoT features (net figures, change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate refined CoT features\n",
    "cot_indicators_spx_refined = refined_cot_data(cot_indicators_spx)\n",
    "cot_indicators_dxa_refined = refined_cot_data(cot_indicators_dxa)\n",
    "cot_indicators_us5yr_refined = refined_cot_data(cot_indicators_us5yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot generated SPX CoT features\n",
    "plot_charts(cot_indicators_spx_refined, dim1 = 3, dim2 = 5,\n",
    "            fig_size = (20, 9), y_label=\"No. of Contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot generated US5YR CoT features\n",
    "plot_charts(cot_indicators_us5yr_refined, dim1 = 3, dim2 = 5,\n",
    "            fig_size = (20, 9), y_label=\"No. of Contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot generated DXA CoT features\n",
    "plot_charts(cot_indicators_dxa_refined, dim1 = 3, dim2 = 5, fig_size = (20, 9), y_label=\"No. of Contracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Import macroeconomic fundamental data and create additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start date: 05/01/1990\n",
    "# read in data\n",
    "m1 = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/M1 v2.csv\") # M1 money supply\n",
    "t10y2y = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/T10Y2Y v2.csv\") # 10 year - 2 year interest rate differential\n",
    "vix = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/VIXCLS.csv\") # VIX index\n",
    "epu = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/EPU.csv\") # Economic policy uncertainty\n",
    "dxy = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/DXY_index.csv\") # US Dollar\n",
    "wti = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/WTI.csv\") # WTI crude oil\n",
    "gld = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/GOLD.csv\") # Gold\n",
    "spy = pd.read_csv(\"C:/Users/Tom/Google Drive/asset-price-prediction/Economic Data/SPY.csv\") # S&P500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the indicators into a single dataframe\n",
    "m1 = m1.drop('DATE', axis = 1)\n",
    "macro_indicators = pd.concat(\n",
    "    [m1, t10y2y], join='outer', axis=1) # concatenate M1 and yield spreads\n",
    "\n",
    "# merge the additional economic series\n",
    "dfs_to_merge = [macro_indicators, vix, dxy, wti, gld, spy, epu]\n",
    "\n",
    "# apply merge to dataframes in list\n",
    "macro_indicators = reduce(lambda df1, df2: \\\n",
    "                          pd.merge(df1,\n",
    "                                   df2,\n",
    "                                   left_on='DATE',\n",
    "                                   right_on='DATE',\n",
    "                                   how='right'), dfs_to_merge)\n",
    "\n",
    "# smooth the series to remove noise (5 week smoothing)\n",
    "macro_indicators['epu_smoothed'] = smoothSeries(epu.EPU, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_transform = ['M1', 'T10Y2Y', 'VIX', 'DXY', 'WTI', 'GLD', 'SPY']\n",
    "\n",
    "# create columns that show 1-week change\n",
    "macro_indicators = create_returns_variables(\n",
    "    df=macro_indicators,\n",
    "    cols=cols_to_transform,\n",
    "    period_weeks=1,\n",
    "    string_append=\"_chg_1w\")\n",
    "\n",
    "# create columns that show 4-week change\n",
    "macro_indicators = create_returns_variables(\n",
    "    df=macro_indicators,\n",
    "    cols=cols_to_transform,\n",
    "    period_weeks=4,\n",
    "    string_append=\"_chg_1m\")\n",
    "\n",
    "# create columns that show 12-week change\n",
    "macro_indicators = create_returns_variables(\n",
    "    df=macro_indicators,\n",
    "    cols=cols_to_transform,\n",
    "    period_weeks=12,\n",
    "    string_append=\"_chg_1q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create binary variables that indicate whether a\n",
    "    features' values are above their moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary indicators that indicates whether VIX and EPU > moving average\n",
    "macro_indicators = create_binary_variables(df=macro_indicators,\n",
    "                                           cols_to_binarise=['VIX', 'EPU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the indicators\n",
    "fig, axs = plt.subplots(1, 2, figsize= (12, 4))\n",
    "fig.tight_layout(pad=3)\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(macro_indicators['VIX'], color='steelblue')\n",
    "axs[0].set_title(\"VIX\")\n",
    "axs[0].plot(macro_indicators['VIX'].rolling(50).mean())\n",
    "axs[0].set_ylabel(\"Value\")\n",
    "axs[0].set_xlabel(\"Weeks\")\n",
    "\n",
    "axs[1].plot(macro_indicators['EPU'], color='steelblue')\n",
    "axs[1].set_title(\"Economic Policy Uncertainty\")\n",
    "axs[1].plot(macro_indicators['EPU'].rolling(50).mean())\n",
    "axs[1].set_ylabel(\"Value\")\n",
    "axs[1].set_xlabel(\"Weeks\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize= (12, 4))\n",
    "fig.tight_layout(pad=3)\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(macro_indicators['VIX_bin'], color='steelblue')\n",
    "axs[0].set_title(\"VIX_bin\")\n",
    "axs[0].set_ylabel(\"Value\")\n",
    "axs[0].set_xlabel(\"Weeks\")\n",
    "\n",
    "axs[1].plot(macro_indicators['EPU_bin'], color='steelblue')\n",
    "axs[1].set_title(\"EPU_bin\")\n",
    "axs[1].set_ylabel(\"Value\")\n",
    "axs[1].set_xlabel(\"Weeks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Seasonal pattern indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoded binary variables to indicate month of the year \n",
    "\n",
    "# dayfirst = True, because original data is in DD/MM/YYYY format\n",
    "macro_indicators['DATE'] = pd.to_datetime(\n",
    "    macro_indicators['DATE'], dayfirst=True)\n",
    "\n",
    "# extract the month from datetime objects\n",
    "macro_indicators['Month'] = macro_indicators['DATE'].dt.month\n",
    "\n",
    "# Create one-hot columns for the months\n",
    "# Set drop_first to true otherwise the dummy trap appears!\n",
    "cols = ['Month']\n",
    "macro_indicators = pd.get_dummies(\n",
    "    macro_indicators, columns=cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create technical indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per Khaidem et al., historical data is exponentially smoothed before creating the technical indicators/features.\n",
    "\n",
    "*\"This smoothing removes random variation or noise from the historical data allowing the model to easily identify long term price trend in the stock price behavior.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the SPX, DXA and US5YR data from bloomberg to calculate technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_path =  \"C:/Users/Tom/Google Drive/asset-price-prediction/BBG data/values/esa_friday.csv\"\n",
    "us5yr_path =  \"C:/Users/Tom/Google Drive/asset-price-prediction/BBG data/values/fva_friday.csv\"\n",
    "dxy_path =  \"C:/Users/Tom/Google Drive/asset-price-prediction/BBG data/values/dxa_friday.csv\"\n",
    "\n",
    "# create indicators for the 3 asset classes\n",
    "technical_indicators_spx = create_technical_indicators(path=spx_path)\n",
    "technical_indicators_us5yr = create_technical_indicators(path=us5yr_path)\n",
    "technical_indicators_dxy = create_technical_indicators(path=dxy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the technical indicators \n",
    "assets = {\n",
    "    'spx': technical_indicators_spx,\n",
    "    'us5yr': technical_indicators_us5yr,\n",
    "    'dxy': technical_indicators_dxy\n",
    "}\n",
    "\n",
    "for a in assets:\n",
    "    plot_charts(assets[a][\n",
    "        ['macd_val', 'rsi', 'williams_r', 'atr', 'change_1',\n",
    "         'change_2', 'change_3', 'change_4', 'change_8']\n",
    "    ], dim1 = 3, dim2 = 4, fig_size=(15,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary technical indicators - indication of trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPX: Add the binary indicators to the 'technical_indicators' dataframe:\n",
    "technical_indicators_spx_binary = create_binary_indicators(\n",
    "    technical_indicators_spx)\n",
    "\n",
    "# US5YR: Add the binary indicators to the 'technical_indicators' dataframe:\n",
    "technical_indicators_us5yr_binary = create_binary_indicators(\n",
    "    technical_indicators_us5yr)\n",
    "\n",
    "# DXA: Add the binary indicators to the 'technical_indicators' dataframe:\n",
    "technical_indicators_dxa_binary = create_binary_indicators(technical_indicators_dxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the newly created binary variables\n",
    "plot_charts(technical_indicators_spx_binary[\n",
    "    ['ema_bin', 'macd_bin', 'ema_cross_bin', 'momentum_bin']\n",
    "], dim1 = 2, dim2 = 2, fig_size=(10,5), y_label=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts(technical_indicators_dxa_binary[\n",
    "    ['ema_bin', 'macd_bin', 'ema_cross_bin', 'momentum_bin']\n",
    "], dim1 = 2, dim2 = 2, fig_size=(10,5), y_label=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts(technical_indicators_us5yr_binary[\n",
    "    ['ema_bin', 'macd_bin', 'ema_cross_bin', 'momentum_bin']\n",
    "], dim1 = 2, dim2 = 2, fig_size=(10,5), y_label=\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Join relevant CoT data, macro data and technical indicators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spx data starts at 1988-06-03 i.e. 3rd June 1988.\n",
    "# CoT data starts at 1988-05-31 i.e. 31st May 1988.\n",
    "\n",
    "# spx data ends at 2018-06-15 i.e. 3rd June 2018.\n",
    "# CoT data ends at 2018-05-22 i.e. 31st May 2018.\n",
    "\n",
    "# append a correct date column to CoT data by\n",
    "\n",
    "# a) removing 1 row from start of CoT\n",
    "cot_indicators_spx_refined = cot_indicators_spx_refined.iloc[1:,:]\n",
    "\n",
    "# b) removing 3 rows from the end of spx Date column\n",
    "date_col = technical_indicators_spx.Date[:-3] # 77 is index for week at which CoT data starts\n",
    "date_col.reset_index(drop=True, inplace=True) # reset the index otherwise concatenate won't work\n",
    "date_col = pd.to_datetime(date_col, dayfirst=True)\n",
    "\n",
    "# c) append the date to CoT indicators\n",
    "cot_indicators_spx = pd.concat(\n",
    "    [date_col, cot_indicators_spx_refined],\n",
    "    join = 'outer', axis=1) # CoT data with date attached\n",
    "\n",
    "# join CoT indicators with macro indicators\n",
    "inputs_spx = pd.merge(\n",
    "    left=cot_indicators_spx,\n",
    "    right=macro_indicators,\n",
    "    left_on='Date',\n",
    "    right_on='DATE',\n",
    "    how = 'outer') \n",
    "\n",
    "inputs_spx = inputs_spx.drop(['DATE'], axis = 1)\n",
    "\n",
    "# join CoT indicators + macro indicators with technical indicators\n",
    "inputs_spx = pd.merge(\n",
    "    inputs_spx,\n",
    "    technical_indicators_spx_binary[\n",
    "        ['Date', 'ema_bin', 'macd_bin', 'ema_cross_bin', 'momentum_bin',\n",
    "         'change_1', 'change_2', 'change_3', 'change_8']], on='Date')\n",
    "\n",
    "# join CoT indicators + macro indicators with technical indicators\n",
    "inputs_spx = pd.merge(inputs_spx, technical_indicators_spx[\n",
    "    ['Date', 'macd_val', 'rsi', 'williams_r', 'atr', 'change_1',\n",
    "     'change_2', 'change_3', 'change_4', 'change_8']\n",
    "], on='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US5YR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us5yr data starts at 1989-07-14 i.e. 14th July 1989.\n",
    "# CoT data starts at 1988-05-31 i.e. 31st May 1988.\n",
    "# us5yr technical ind. starts at 1988-07-14\n",
    "\n",
    "# us5yr data ends at 2018-06-15 i.e. 29 June 2018.\n",
    "# CoT data ends at 2018-05-22 i.e. 22nd May 2018.\n",
    "# us5yr technical ind. ends at 2018-06-29\n",
    "\n",
    "# append a correct date column to CoT data by\n",
    "\n",
    "# a) removing n rows from start of CoT to line up with us5yr data... 14 July 1989 is index 58 in us5yr_cot\n",
    "\n",
    "cot_indicators_us5yr_refined = cot_indicators_us5yr_refined.iloc[58:,:]\n",
    "cot_indicators_us5yr_refined= cot_indicators_us5yr_refined.reset_index(drop=True)\n",
    "\n",
    "# b) removing 4 rows from the end of us5yr Date column\n",
    "date_col = technical_indicators_us5yr.Date[:-5]\n",
    "date_col.reset_index(drop=True, inplace=True) # reset the index otherwise concatenate won't work\n",
    "date_col = pd.to_datetime(date_col, dayfirst=True)\n",
    "date_col = pd.DataFrame(date_col)\n",
    "\n",
    "# c) append the date to CoT indicators\n",
    "cot_indicators_us5yr = pd.concat(\n",
    "    [date_col, cot_indicators_us5yr_refined],\n",
    "    join = 'outer',\n",
    "    axis=1) # CoT data with date attached\n",
    "\n",
    "inputs_us5yr = pd.merge(\n",
    "    left=cot_indicators_us5yr,\n",
    "    right=macro_indicators,\n",
    "    left_on='Date',\n",
    "    right_on='DATE',\n",
    "    how = 'outer') # join CoT indicators with macro indicators\n",
    "\n",
    "inputs_us5yr = inputs_us5yr.drop(['DATE'], axis = 1)\n",
    "\n",
    "# join CoT indicators + macro indicators with technical indicators\n",
    "inputs_us5yr = pd.merge(\n",
    "    inputs_us5yr,\n",
    "    technical_indicators_us5yr_binary[\n",
    "        ['Date', 'ema_bin', 'macd_bin', 'ema_cross_bin', 'momentum_bin',\n",
    "         'change_1', 'change_2', 'change_3', 'change_8']], on='Date')\n",
    "\n",
    "# join CoT indicators + macro indicators with technical indicators\n",
    "inputs_us5yr = pd.merge(\n",
    "    inputs_us5yr,\n",
    "    technical_indicators_us5yr[\n",
    "        ['Date', 'macd_val', 'rsi', 'williams_r', 'atr', 'change_1',\n",
    "         'change_2', 'change_3', 'change_4', 'change_8']], on='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dxa data starts at 1988-06-03 i.e. 3rd June 1988.\n",
    "# CoT data starts at 1988-05-31 i.e. 31st May 1988.\n",
    "\n",
    "# dxa data ends at 2018-06-15 i.e. 3rd June 2018.\n",
    "# CoT data ends at 2018-05-22 i.e. 31st May 2018.\n",
    "\n",
    "# append a correct date column to CoT data by\n",
    "\n",
    "# a) removing 1 row from start of CoT\n",
    "cot_indicators_dxa_refined = cot_indicators_dxa_refined.iloc[1:,:]\n",
    "\n",
    "# b) removing 3 rows from the end of dxa Date column\n",
    "date_col = technical_indicators_dxy.Date[:-3] # 77 is index for week at which CoT data starts\n",
    "date_col.reset_index(drop=True, inplace=True) # reset the index otherwise concatenate won't work\n",
    "date_col = pd.to_datetime(date_col, dayfirst=True)\n",
    "\n",
    "# c) append the date to CoT indicators\n",
    "cot_indicators_dxa = pd.concat(\n",
    "    [date_col, cot_indicators_dxa_refined],\n",
    "    join = 'outer', axis=1) # CoT data with date attached\n",
    "\n",
    "inputs_dxa = pd.merge(\n",
    "    left=cot_indicators_dxa,\n",
    "    right=macro_indicators,\n",
    "    left_on='Date',\n",
    "    right_on='DATE',\n",
    "    how = 'outer') # join CoT indicators with macro indicators\n",
    "\n",
    "inputs_dxa = inputs_dxa.drop(['DATE'], axis = 1)\n",
    "\n",
    "# join CoT indicators + macro indicators with technical indicators\n",
    "inputs_dxa = pd.merge(inputs_dxa, technical_indicators_dxa_binary[\n",
    "    ['Date', 'ema_bin', 'macd_bin', 'ema_cross_bin', 'momentum_bin',\n",
    "     'change_1', 'change_2', 'change_3', 'change_8']], on='Date')\n",
    "\n",
    "# join CoT indicators + macro indicators with technical indicators\n",
    "inputs_dxa = pd.merge(inputs_dxa, technical_indicators_dxy[\n",
    "    ['Date', 'macd_val', 'rsi', 'williams_r', 'atr', 'change_1',\n",
    "     'change_2', 'change_3', 'change_4', 'change_8']], on='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 create lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns for which we want to lag\n",
    "cols = ['net_comm', 'net_noncomm', 'net_nonrep', 'net_comm_chg_1w',\n",
    "        'net_noncomm_chg_1w', 'net_nonrep_chg_1w', 'net_comm_chg_1m',\n",
    "        'net_noncomm_chg_1m', 'net_nonrep_chg_1m', 'net_comm_chg_1q',\n",
    "        'net_noncomm_chg_1q', 'net_nonrep_chg_1q', 'net_comm_long',\n",
    "        'net_noncomm_long', 'net_nonrep_long', 'M1', 'T10Y2Y', 'VIX',\n",
    "        'EPU', 'DXY', 'SPY', 'epu_smoothed', 'M1_chg_1w', 'T10Y2Y_chg_1w',\n",
    "        'VIX_chg_1w', 'M1_chg_1m', 'T10Y2Y_chg_1m', 'VIX_chg_1m',\n",
    "        'M1_chg_1q', 'T10Y2Y_chg_1q', 'VIX_chg_1q', 'DXY_chg_1w',\n",
    "        'DXY_chg_1m', 'DXY_chg_1q', 'SPY_chg_1w', 'SPY_chg_1m',\n",
    "        'SPY_chg_1q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_spx = create_lagged_features(df=inputs_spx, cols_to_lag=cols)\n",
    "inputs_us5yr = create_lagged_features(df=inputs_us5yr, cols_to_lag=cols)\n",
    "inputs_dxa = create_lagged_features(df=inputs_dxa, cols_to_lag=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Generate target variables\n",
    "Binary variables indicating whether asset price rose or fell over a defined period (default is 4 weeks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_target = create_target_variable(\n",
    "    path=\"C:/Users/Tom/Google Drive/asset-price-prediction/BBG data/values/esa_friday.csv\",\n",
    "    weeks=4)\n",
    "\n",
    "fva_target = create_target_variable(\n",
    "    path=\"C:/Users/Tom/Google Drive/asset-price-prediction/BBG data/values/fva_friday.csv\",\n",
    "    weeks=4)\n",
    "\n",
    "dxa_target = create_target_variable(\n",
    "    path=\"C:/Users/Tom/Google Drive/asset-price-prediction/BBG data/values/dxa_friday.csv\",\n",
    "    weeks=4)\n",
    "\n",
    "# join the targets to the input data on date\n",
    "inputs_spx = pd.merge(inputs_spx, spx_target, on='Date').dropna(how='any', axis=0)\n",
    "inputs_us5yr = pd.merge(inputs_us5yr, fva_target, on='Date').dropna(how='any', axis=0)\n",
    "inputs_dxa = pd.merge(inputs_dxa, dxa_target, on='Date').dropna(how='any', axis=0)\n",
    "\n",
    "# remove columns that might give classifier the answer\n",
    "# for example, DXA price might help SPX prediction, but shouldn't be in DXA inputs\n",
    "# identify columns with DXY\n",
    "cols_to_drop = inputs_spx.filter(like='SPY').columns\n",
    "# drop them\n",
    "inputs_spx= inputs_spx.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# identify columns with DXY\n",
    "cols_to_drop = inputs_dxa.filter(like='DXY').columns\n",
    "# drop them\n",
    "inputs_dxa= inputs_dxa.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Drop columns with _y for all assets\n",
    "cols_to_drop = inputs_spx.filter(like='_y').columns\n",
    "inputs_spx= inputs_spx.drop(cols_to_drop, axis=1)\n",
    "\n",
    "cols_to_drop = inputs_us5yr.filter(like='_y').columns\n",
    "inputs_us5yr= inputs_us5yr.drop(cols_to_drop, axis=1)\n",
    "\n",
    "cols_to_drop = inputs_dxa.filter(like='_y').columns\n",
    "inputs_dxa= inputs_dxa.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up X and y matrices and vectors for next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spx, y_spx = create_x_y(df=inputs_spx)\n",
    "plot_class_frequencies(y_spx)\n",
    "\n",
    "X_us5yr, y_us5yr = create_x_y(df=inputs_us5yr)\n",
    "plot_class_frequencies(y_us5yr)\n",
    "\n",
    "X_dxa, y_dxa = create_x_y(df=inputs_dxa)\n",
    "plot_class_frequencies(y_dxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store inputs_spx\n",
    "%store inputs_us5yr\n",
    "%store inputs_dxa\n",
    "\n",
    "%store X_spx\n",
    "%store X_us5yr\n",
    "%store X_dxa\n",
    "\n",
    "%store y_spx\n",
    "%store y_us5yr\n",
    "%store y_dxa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
